{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dt import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanBaseEstimator:\n",
    "    def fit(self, X, y):\n",
    "        self.avg = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.empty(X.shape[0], dtype=np.float64)\n",
    "        pred.fill(self.avg)\n",
    "        return pred\n",
    "\n",
    "class MSELoss:\n",
    "    def __call__(self, y, y_pred):\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    def base_estimator(self):\n",
    "        return MeanBaseEstimator()\n",
    "\n",
    "    def grad(self, y, y_pred):\n",
    "        # simple derivative\n",
    "        return -2 / len(y) * (y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class GradientBoostedDecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_iter,\n",
    "        max_depth=None,\n",
    "        learning_rate=1,\n",
    "        step_size=\"constant\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A gradient boosted ensemble of decision trees.\n",
    "        Notes\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_iter : int\n",
    "            The number of iterations / weak estimators to use when fitting each\n",
    "            dimension / class of `Y`.\n",
    "        max_depth : int\n",
    "            The maximum depth of each decision tree weak estimator. Default is\n",
    "            None.\n",
    "        classifier : bool\n",
    "            Whether `Y` contains class labels or real-valued targets. Default\n",
    "            is True.\n",
    "        learning_rate : float\n",
    "            Value in [0, 1] controlling the amount each weak estimator\n",
    "            contributes to the overall model prediction. Sometimes known as the\n",
    "            `shrinkage parameter` in the GBM literature. Default is 1.\n",
    "        loss : {'crossentropy', 'mse'}\n",
    "            The loss to optimize for the GBM. Default is 'crossentropy'.\n",
    "        step_size : {\"constant\", \"adaptive\"}\n",
    "            How to choose the weight for each weak learner. If \"constant\", use\n",
    "            a fixed weight of 1 for each learner. If \"adaptive\", use a step\n",
    "            size computed via line-search on the current iteration's loss.\n",
    "            Default is 'constant'.\n",
    "        \"\"\"\n",
    "        self.weights = None\n",
    "        self.learners = None\n",
    "        self.out_dims = None\n",
    "        self.n_iter = n_iter\n",
    "        self.base_estimator = None\n",
    "        self.max_depth = max_depth\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit the gradient boosted decision trees on a dataset.\n",
    "        \"\"\"\n",
    "        # set loss function\n",
    "        loss = MSELoss()\n",
    "        # if Y array has only one dimension then make sure to get array with two dimensions\n",
    "        # where the first one can be whatever fits and the secon one has to be 1.\n",
    "        Y = Y.reshape(-1, 1) if len(Y.shape) == 1 else Y\n",
    "        N, M = X.shape\n",
    "        # this is usually 1\n",
    "        self.out_dims = Y.shape[1]\n",
    "        # each iteration is one row of learners with the same length as the output y\n",
    "        # so basically matrix with n_iter rows and out_dims columns which is most often 1.\n",
    "        self.learners = np.empty((self.n_iter, self.out_dims), dtype=object)\n",
    "        # the same is valid for the weights\n",
    "        # as we have a weight for each learner\n",
    "        self.weights = np.ones((self.n_iter, self.out_dims))\n",
    "        # all but the first (i.e. zero positioned) row\n",
    "        self.weights[1:, :] *= self.learning_rate\n",
    "\n",
    "        # fit the base estimator\n",
    "        Y_pred = np.zeros((N, self.out_dims))\n",
    "        # this usually sets the first learner to the value of the base estimator\n",
    "        # for us this would be the average of the Y values\n",
    "        # removed k loop from original\n",
    "        \n",
    "        # this calls the MeanBaseEstimator() in the case of MSELoss\n",
    "        # or the ClassProbEstimator() in the case of CrossEntropyLoss\n",
    "        # here we consider only MSE loss\n",
    "        t = loss.base_estimator()\n",
    "        # in our case this takes the mean/avg of Y's column k\n",
    "        t.fit(X, Y[:, 0])\n",
    "        # now we predict the values by adding the vector of means\n",
    "        # onto the Y_pred column vector\n",
    "        Y_pred[:, 0] += t.predict(X)\n",
    "        # the prediction is just the avg value\n",
    "        # now we save the base estimator to the zeroth row and k-th column of the learners\n",
    "        # which contains one row for each iteration\n",
    "        self.learners[0, 0] = t\n",
    "\n",
    "        # incrementally fit each learner on the negative gradient of the loss\n",
    "        # wrt the previous fit (pseudo-residuals)\n",
    "        for i in range(1, self.n_iter):\n",
    "            # out dims is usually 1 so removed it \n",
    "\n",
    "            y, y_pred = Y[:, 0], Y_pred[:, 0]\n",
    "            # use derivative of MSE loss to obtain negative gradient\n",
    "            neg_grad = -1 * loss.grad(y, y_pred)\n",
    "            \n",
    "            # take decision tree discussed in previous post\n",
    "            # use MSE as the surrogate loss when fitting to negative gradients\n",
    "            t = DecisionTree(\n",
    "                classifier=False, max_depth=self.max_depth, criterion=\"mse\"\n",
    "            )\n",
    "\n",
    "            # fit current learner to negative gradients\n",
    "            t.fit(X, neg_grad)\n",
    "            # save trained learner for each iteration\n",
    "            self.learners[i, 0] = t\n",
    "\n",
    "            # compute step size and weight for the current learner\n",
    "            step = 1.0\n",
    "            h_pred = t.predict(X)\n",
    "\n",
    "            # update weights and our overall prediction for Y\n",
    "            self.weights[i, 0] *= step\n",
    "            Y_pred[:, 0] += self.weights[i, 0] * h_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to classify or predict the examples in `X`.\n",
    "        Parameters \n",
    "        \"\"\"\n",
    "        Y_pred = np.zeros((X.shape[0], self.out_dims))\n",
    "        for i in range(self.n_iter):\n",
    "            # removed k loop from original\n",
    "            Y_pred[:, 0] += self.weights[i, 0] * self.learners[i, 0].predict(X)\n",
    "\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.random.uniform(0, 100, 100)\n",
    "X = np.random.uniform(0, 100, (100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.93102011],\n",
       "       [68.77078514],\n",
       "       [75.75057859],\n",
       "       [78.93915104],\n",
       "       [29.70717605],\n",
       "       [67.13328175],\n",
       "       [68.05603813],\n",
       "       [29.81417086],\n",
       "       [27.70733316],\n",
       "       [40.99513131],\n",
       "       [ 6.68114214],\n",
       "       [80.00674184],\n",
       "       [10.08887845],\n",
       "       [51.41639291],\n",
       "       [41.49629209],\n",
       "       [85.00323326],\n",
       "       [20.45265419],\n",
       "       [14.15023088],\n",
       "       [23.30708667],\n",
       "       [22.09649838],\n",
       "       [10.46878876],\n",
       "       [63.17523835],\n",
       "       [28.11963647],\n",
       "       [ 8.94342996],\n",
       "       [68.25249424],\n",
       "       [83.88226478],\n",
       "       [10.04116925],\n",
       "       [44.33899683],\n",
       "       [48.5088378 ],\n",
       "       [45.00627327],\n",
       "       [15.80353042],\n",
       "       [18.88602579],\n",
       "       [19.93964957],\n",
       "       [34.74207532],\n",
       "       [35.54844761],\n",
       "       [52.99488511],\n",
       "       [38.81251851],\n",
       "       [41.52990788],\n",
       "       [ 9.0703191 ],\n",
       "       [66.93066188],\n",
       "       [86.67453959],\n",
       "       [64.29881549],\n",
       "       [82.03537414],\n",
       "       [ 8.56565582],\n",
       "       [11.73444802],\n",
       "       [72.50317026],\n",
       "       [34.01570764],\n",
       "       [43.79116263],\n",
       "       [32.93201634],\n",
       "       [27.65427925],\n",
       "       [18.68370495],\n",
       "       [80.39318086],\n",
       "       [16.5131418 ],\n",
       "       [71.39007307],\n",
       "       [79.25482845],\n",
       "       [47.11964521],\n",
       "       [68.03715789],\n",
       "       [81.70177852],\n",
       "       [49.10278584],\n",
       "       [87.54341622],\n",
       "       [37.22080779],\n",
       "       [77.27503615],\n",
       "       [71.38773036],\n",
       "       [75.55259586],\n",
       "       [57.5980818 ],\n",
       "       [32.06593927],\n",
       "       [ 9.41737515],\n",
       "       [42.78052281],\n",
       "       [13.70847359],\n",
       "       [ 9.58397869],\n",
       "       [56.44053506],\n",
       "       [84.81490916],\n",
       "       [92.82316242],\n",
       "       [86.53313583],\n",
       "       [76.59447107],\n",
       "       [12.29605478],\n",
       "       [73.96432124],\n",
       "       [51.83932643],\n",
       "       [83.52953138],\n",
       "       [71.00294822],\n",
       "       [85.66144578],\n",
       "       [43.04762786],\n",
       "       [46.03086552],\n",
       "       [91.70882719],\n",
       "       [71.34596726],\n",
       "       [65.51262461],\n",
       "       [77.29440612],\n",
       "       [84.25792853],\n",
       "       [60.34682476],\n",
       "       [74.02692149],\n",
       "       [24.91516312],\n",
       "       [37.03877089],\n",
       "       [29.24525422],\n",
       "       [56.45053184],\n",
       "       [58.55958292],\n",
       "       [52.33300012],\n",
       "       [21.95440857],\n",
       "       [60.49144636],\n",
       "       [24.78684497],\n",
       "       [13.46207336]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = GradientBoostedDecisionTree(n_iter=100)\n",
    "t.fit(X,Y)\n",
    "t.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('practice_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15e5909eea482f02c1be172d7d1d87213b3e75007f621194ff3400df51bcbb77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
