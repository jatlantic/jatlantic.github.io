{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the code\n",
    "\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Information Entropy\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Gini Impurity\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        \"\"\"\n",
    "        `value` is an array of class probabilities if classifier is True, else\n",
    "        the mean of the region\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion = \"entropy\",\n",
    "        n_feats = None\n",
    "    ):\n",
    "        self.criterion = criterion\n",
    "        self.depth = 0\n",
    "        self.max_depth = np.inf\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit a binary decision tree to a dataset.\n",
    "        Parameters\n",
    "        \"\"\"\n",
    "        # determine how many features we consider for each splits (n_feats)\n",
    "        self.n_feats = X.shape[1]\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "\n",
    "        \n",
    "    def _grow(self, X, Y, cur_depth=0):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(set(Y)) == 1:\n",
    "            return Leaf(Y[0])\n",
    "\n",
    "        cur_depth += 1\n",
    "        self.depth = max(self.depth, cur_depth)\n",
    "\n",
    "        N, M = X.shape\n",
    "\n",
    "        # generate a random sample from the 1-D array of M columns without replacement\n",
    "        feat_idxs = np.random.choice(M, size = self.n_feats, replace = False)\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        # apply segment function\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # apply the same as before one levael down in the tree\n",
    "        # grow the children that result from the split\n",
    "        left = self._grow(X[l, :], Y[l], cur_depth)\n",
    "        right = self._grow(X[r, :], Y[r], cur_depth)\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        \"\"\"\n",
    "        Find the optimal split rule (feature index and split threshold) for the\n",
    "        data according to `self.criterion`.\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            # get the i-th column which we created before with the number generator\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            # [:-1] -> take all but the last element\n",
    "            # [1:] -> take all but the first element\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2 if len(levels) > 1 else levels\n",
    "            # now call function impurity gain\n",
    "            # collect all values of the gain in list (for each threshold)\n",
    "            gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n",
    "\n",
    "            # update the gain if new one is btter\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                # save the threshold for the max gain\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, split_thresh, feat_values):\n",
    "        \"\"\"\n",
    "        Compute the impurity gain associated with a given split.\n",
    "        IG(split) = loss(parent) - weighted_avg[loss(left_child), loss(right_child)]\n",
    "        \"\"\"\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "\n",
    "        # calculate the loss of the parent before the split\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate splits by getting all index positions where the value is <= or > than the threshold\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        # number of indices making the threshold\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        \n",
    "        # calculate loss according evaluation criterion defined\n",
    "        # taking the rows by left and right index of Y as the input\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # impurity gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        print('x', X)\n",
    "        if isinstance(node, Leaf):\n",
    "            print(node.value)\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            print('feature:',node.feature)\n",
    "            print('threshold',node.threshold)\n",
    "            #print('node left',node.left)\n",
    "            #print('node right',node.right)\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to classify or predict the examples in `X`.\n",
    "        \"\"\"\n",
    "        # iterates over rows of X!\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 8 5 9]\n",
      "x [ 2 -1  5  6]\n",
      "x [ 2 -1  5  6]\n",
      "feature: 0\n",
      "threshold 3.0\n",
      "x [ 2 -1  5  6]\n",
      "2\n",
      "x [ 4 -2  3  0]\n",
      "x [ 4 -2  3  0]\n",
      "x [ 4 -2  3  0]\n",
      "8\n",
      "x [3 0 1 5]\n",
      "feature: 2\n",
      "threshold 2.5\n",
      "x [3 0 1 5]\n",
      "x [3 0 1 5]\n",
      "5\n",
      "x [-1  1  2  2]\n",
      "feature: 2\n",
      "threshold 2.5\n",
      "x [-1  1  2  2]\n",
      "feature: 0\n",
      "threshold 1.0\n",
      "x [-1  1  2  2]\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 8, 5, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTree()\n",
    "X = np.array([[2,-1,5,6],[4,-2,3,0],[3,0,1,5], [-1,1,2,2]])\n",
    "Y = np.random.choice(10,4)\n",
    "print(Y)\n",
    "dt.fit(X,Y)\n",
    "dt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('practice_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15e5909eea482f02c1be172d7d1d87213b3e75007f621194ff3400df51bcbb77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
